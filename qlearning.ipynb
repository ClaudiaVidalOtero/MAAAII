{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b247166d",
   "metadata": {},
   "source": [
    "\n",
    "Practica 3 (Pendulum-v1 versión Q-Learning)\n",
    "\n",
    "Aldana Smyna Medina Lostaunau\n",
    "\n",
    "Claudia Vidal Otero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ce62b",
   "metadata": {},
   "source": [
    "IMPORTACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a86171",
   "metadata": {},
   "source": [
    "CONFIGURACIÓN DE HIPERPARÁMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Pendulum-v1'\n",
    "RENDER = False\n",
    "PERTURBACIONES = False\n",
    "PERTURB_PROB = 0.05 #Pedida por el enunciado\n",
    "PERTURB_VALS = [-2.0, 2.0]  #Pedidos por el enunciado\n",
    "\n",
    "N_ANGLE_BINS = 30 #\n",
    "N_VELOCITY_BINS = 15 #\n",
    "N_ACTIONS = 11\n",
    "\n",
    "\n",
    "ANGLE_MIN, ANGLE_MAX = -np.pi, np.pi\n",
    "VEL_MIN, VEL_MAX = -8.0, 8.0\n",
    "ACTION_MIN, ACTION_MAX = -2.0, 2.0\n",
    "\n",
    "NUM_EPISODES = 20000\n",
    "MAX_STEPS = 200\n",
    "GAMMA = 0.95\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_END = 0.01\n",
    "ALPHA = 0.1\n",
    "\n",
    "\n",
    "\n",
    "angle_bins = np.linspace(ANGLE_MIN, ANGLE_MAX, N_ANGLE_BINS + 1)\n",
    "vel_bins = np.linspace(VEL_MIN, VEL_MAX, N_VELOCITY_BINS + 1)\n",
    "action_list = np.linspace(ACTION_MIN, ACTION_MAX, N_ACTIONS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11247721",
   "metadata": {},
   "source": [
    "\n",
    "FUNCIONES AUXILIARES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b418e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discretize_state(obs):\n",
    "    cos_theta, sin_theta, theta_dot = obs\n",
    "    theta = np.arctan2(sin_theta, cos_theta)\n",
    "    angle_idx = np.digitize(theta, angle_bins) - 1\n",
    "    vel_idx = np.digitize(theta_dot, vel_bins) - 1\n",
    "    angle_idx = np.clip(angle_idx, 0, N_ANGLE_BINS - 1)\n",
    "    vel_idx = np.clip(vel_idx, 0, N_VELOCITY_BINS - 1)\n",
    "    return (angle_idx, vel_idx)\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(N_ACTIONS)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def apply_perturbation(action):\n",
    "    if np.random.rand() < PERTURB_PROB:\n",
    "        return [np.random.choice(PERTURB_VALS)]\n",
    "    else:\n",
    "        return [action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6495e23",
   "metadata": {},
   "source": [
    "ALGORITMO DE Q-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def q_learning(env):\n",
    "    Q = defaultdict(lambda: np.zeros(N_ACTIONS))\n",
    "    returns = []\n",
    "    epsilon = EPSILON_START\n",
    "\n",
    "    for ep in range(1, NUM_EPISODES + 1):\n",
    "        obs, _ = env.reset()\n",
    "        state = discretize_state(obs)\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(MAX_STEPS):\n",
    "            action_idx = epsilon_greedy(Q, state, epsilon)\n",
    "            action = action_list[action_idx]\n",
    "            if PERTURBACIONES:\n",
    "                action = apply_perturbation(action)[0]\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step([action])\n",
    "            next_state = discretize_state(next_obs)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + GAMMA * Q[next_state][best_next_action]\n",
    "            td_error = td_target - Q[state][action_idx]\n",
    "            Q[state][action_idx] += ALPHA * td_error\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        returns.append(total_reward)\n",
    "        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "\n",
    "        if ep % 500 == 0:\n",
    "            print(f\"Episodio {ep}/{NUM_EPISODES}, retorno medio últimos 500: {np.mean(returns[-500:]):.2f}\")\n",
    "\n",
    "    return Q, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42837507",
   "metadata": {},
   "source": [
    "EVALUACIÓN DE POLÍTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3935e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_politica(policy_det, n_episodios=1000):\n",
    "    env_eval = gym.make(ENV_NAME, render_mode='human' if RENDER else None)\n",
    "    retornos = []\n",
    "\n",
    "    for _ in range(n_episodios):\n",
    "        obs, _ = env_eval.reset()\n",
    "        state = discretize_state(obs)\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(MAX_STEPS):\n",
    "            action_idx = policy_det.get(state, np.random.choice(N_ACTIONS))\n",
    "            action = action_list[action_idx]\n",
    "            obs, reward, terminated, truncated, _ = env_eval.step([action])\n",
    "            total_reward += reward\n",
    "            state = discretize_state(obs)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        retornos.append(total_reward)\n",
    "\n",
    "    env_eval.close()\n",
    "    return np.mean(retornos), np.std(retornos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49978ab",
   "metadata": {},
   "source": [
    "VISUALIZACIÓN DE POLÍTICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_politica(policy_det, n_episodios=5):\n",
    "    env_vis = gym.make(ENV_NAME, render_mode='human')\n",
    "\n",
    "    for ep in range(n_episodios):\n",
    "        obs, _ = env_vis.reset()\n",
    "        state = discretize_state(obs)\n",
    "        total_reward = 0\n",
    "        print(f\"\\n[EPISODIO {ep+1}]\")\n",
    "\n",
    "        for t in range(MAX_STEPS):\n",
    "            action_idx = policy_det.get(state, np.random.choice(N_ACTIONS))\n",
    "            action = action_list[action_idx]\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env_vis.step([action])\n",
    "            total_reward += reward\n",
    "            state = discretize_state(obs)\n",
    "\n",
    "            env_vis.render()\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        print(f\"Retorno del episodio {ep+1}: {total_reward:.2f}\")\n",
    "\n",
    "    env_vis.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e38079",
   "metadata": {},
   "source": [
    "EJECUCIÓN ALGORITMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(ENV_NAME, render_mode=None)\n",
    "Q, returns = q_learning(env)\n",
    "\n",
    "policy_det = {s: np.argmax(a) for s, a in Q.items()}\n",
    "\n",
    "os.makedirs('resultados', exist_ok=True)\n",
    "\n",
    "# Guardar gráfico\n",
    "plt.plot(returns)\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Retorno')\n",
    "plt.title('Q-Learning - Pendulum-v1')\n",
    "plt.savefig('resultados/grafico_qlearning.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0a496",
   "metadata": {},
   "source": [
    "EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69de506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar\n",
    "media, std = evaluar_politica(policy_det)\n",
    "print(f\"\\nEvaluación política final: Retorno medio = {media:.2f}, Std = {std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c719de",
   "metadata": {},
   "source": [
    "VISUALIZACIÓN FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cd352",
   "metadata": {},
   "source": [
    "# Visualizar política entrenada\n",
    "visualizar_politica(policy_det, n_episodios=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
