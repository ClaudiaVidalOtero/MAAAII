{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pr√°ctica de Aprendizaje Semi-Supervisado con CIFAR-100\n",
    "\n",
    "En esta pr√°ctica se desarrollan distintos enfoques para el aprendizaje semi-supervisado sobre el conjunto de datos CIFAR-100. Se parte de un conjunto de 50.000 instancias de entrenamiento y 10.000 instancias de prueba (etiquetadas en 100 clases). Se procede a eliminar el 80% de las etiquetas en el conjunto de entrenamiento, obteni√©ndose:\n",
    "- 10.000 instancias etiquetadas\n",
    "- 40.000 instancias sin etiquetar\n",
    "\n",
    "A continuaci√≥n se detallan los ejercicios a desarrollar:\n",
    "1. Entrenar un modelo (con al menos 4 capas densas y/o convolucionales) utilizando √∫nicamente los datos etiquetados.\n",
    "2. Entrenar el mismo modelo usando auto-aprendizaje (self-training) para incorporar los datos sin etiquetar.\n",
    "3. Entrenar un modelo semi-supervisado tipo autoencoder en dos pasos: primero entrenar el autoencoder (utilizando la misma arquitectura encoder que en 1 y 2, salvo el √∫ltimo bloque) y luego entrenar el clasificador.\n",
    "4. Entrenar un modelo semi-supervisado de tipo autoencoder en un √∫nico paso (reconstrucci√≥n y clasificaci√≥n simult√°nea).\n",
    "5. Repetir los entrenamientos anteriores eliminando aquellas instancias no etiquetadas at√≠picas (seg√∫n la t√©cnica explicada en el Notebook 5, usando un valor de ùë£ = 0.9).\n",
    "6. Repetir los Ejercicios 3‚Äì5 usando la t√©cnica del apartado ‚ÄúHay vida m√°s all√° del autoencoder‚Äù (manteniendo la misma arquitectura encoder).\n",
    "\n",
    "## Preparaci√≥n: Carga y separaci√≥n de datos\n",
    "\n",
    "Utilizaremos la utilidad de Keras para cargar el conjunto de datos CIFAR-100. Posteriormente se separar√° el conjunto de entrenamiento en:\n",
    "- 10.000 instancias etiquetadas.\n",
    "- 40.000 instancias sin etiqueta (se ignoran las etiquetas originales para el entrenamiento semi-supervisado).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos etiquetados: (10000, 32, 32, 3) (10000, 100)\n",
      "Datos sin etiquetar: (40000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar CIFAR-100\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "# Normalizaci√≥n de im√°genes\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convertir etiquetas a one-hot para la clasificaci√≥n\n",
    "y_train_cat = to_categorical(y_train, 100)\n",
    "y_test_cat  = to_categorical(y_test, 100)\n",
    "\n",
    "# Definir n√∫mero de datos etiquetados (10k) y sin etiquetar (40k)\n",
    "n_labeled = 10000\n",
    "n_total = x_train.shape[0]\n",
    "\n",
    "# Mezclar aleatoriamente el conjunto de entrenamiento\n",
    "indices = np.arange(n_total)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Seleccionar √≠ndices para etiquetado y sin etiquetar\n",
    "labeled_indices = indices[:n_labeled]\n",
    "unlabeled_indices = indices[n_labeled:]\n",
    "\n",
    "x_train_labeled = x_train[labeled_indices]\n",
    "y_train_labeled = y_train_cat[labeled_indices]\n",
    "\n",
    "x_train_unlabeled = x_train[unlabeled_indices]\n",
    "# Nota: Las etiquetas originales de x_train_unlabeled se ignoran para el auto-aprendizaje\n",
    "\n",
    "print(\"Datos etiquetados:\", x_train_labeled.shape, y_train_labeled.shape)\n",
    "print(\"Datos sin etiquetar:\", x_train_unlabeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio 1: Modelo supervisado utilizando √∫nicamente los datos etiquetados\n",
    "\n",
    "En este ejercicio se define y entrena un modelo de TensorFlow basado en una arquitectura que combina capas convolucionales y densas (al menos 4 bloques) utilizando solo los 10.000 ejemplos etiquetados.\n",
    "\n",
    "### Preguntas a responder:\n",
    "a. **¬øQu√© red has escogido? ¬øPor qu√©? ¬øC√≥mo la has entrenado?**  \n",
    "   Se ha optado por una red CNN simple que consta de dos bloques convolucionales seguidos de capas densas. Se eligi√≥ esta arquitectura por su eficacia en tareas de clasificaci√≥n de im√°genes y su relativa simplicidad.\n",
    "\n",
    "b. **¬øCu√°l es el rendimiento del modelo en entrenamiento? ¬øY en prueba?**  \n",
    "   Se reportar√°n las m√©tricas de p√©rdida y precisi√≥n tanto en el conjunto de entrenamiento como en el de prueba.\n",
    "\n",
    "c. **Conclusiones**  \n",
    "   Se analizar√°n las limitaciones al usar pocos datos etiquetados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\108057\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 - 13s - 83ms/step - accuracy: 0.0303 - loss: 4.4192 - val_accuracy: 0.0483 - val_loss: 4.2447\n",
      "Epoch 2/20\n",
      "157/157 - 9s - 54ms/step - accuracy: 0.0806 - loss: 3.9827 - val_accuracy: 0.0964 - val_loss: 3.8884\n",
      "Epoch 3/20\n",
      "157/157 - 8s - 48ms/step - accuracy: 0.1488 - loss: 3.5908 - val_accuracy: 0.1615 - val_loss: 3.6003\n",
      "Epoch 4/20\n",
      "157/157 - 8s - 48ms/step - accuracy: 0.2138 - loss: 3.2442 - val_accuracy: 0.2085 - val_loss: 3.3188\n",
      "Epoch 5/20\n",
      "157/157 - 8s - 53ms/step - accuracy: 0.2730 - loss: 2.9472 - val_accuracy: 0.2223 - val_loss: 3.2691\n",
      "Epoch 6/20\n",
      "157/157 - 9s - 55ms/step - accuracy: 0.3281 - loss: 2.6708 - val_accuracy: 0.2370 - val_loss: 3.2519\n",
      "Epoch 7/20\n",
      "157/157 - 8s - 49ms/step - accuracy: 0.3864 - loss: 2.3965 - val_accuracy: 0.2537 - val_loss: 3.1518\n",
      "Epoch 8/20\n",
      "157/157 - 8s - 53ms/step - accuracy: 0.4430 - loss: 2.1106 - val_accuracy: 0.2586 - val_loss: 3.2437\n",
      "Epoch 9/20\n",
      "157/157 - 9s - 55ms/step - accuracy: 0.5044 - loss: 1.8417 - val_accuracy: 0.2649 - val_loss: 3.2904\n",
      "Epoch 10/20\n",
      "157/157 - 10s - 66ms/step - accuracy: 0.5794 - loss: 1.5417 - val_accuracy: 0.2689 - val_loss: 3.4069\n",
      "Epoch 11/20\n",
      "157/157 - 10s - 62ms/step - accuracy: 0.6492 - loss: 1.2686 - val_accuracy: 0.2648 - val_loss: 3.7189\n",
      "Epoch 12/20\n",
      "157/157 - 9s - 60ms/step - accuracy: 0.7239 - loss: 1.0049 - val_accuracy: 0.2639 - val_loss: 3.9959\n",
      "Epoch 13/20\n",
      "157/157 - 9s - 60ms/step - accuracy: 0.7828 - loss: 0.7768 - val_accuracy: 0.2632 - val_loss: 4.3491\n",
      "Epoch 14/20\n",
      "157/157 - 9s - 56ms/step - accuracy: 0.8430 - loss: 0.5608 - val_accuracy: 0.2588 - val_loss: 4.7518\n",
      "Epoch 15/20\n",
      "157/157 - 9s - 55ms/step - accuracy: 0.8857 - loss: 0.4099 - val_accuracy: 0.2575 - val_loss: 5.3825\n",
      "Epoch 16/20\n",
      "157/157 - 9s - 55ms/step - accuracy: 0.9050 - loss: 0.3330 - val_accuracy: 0.2614 - val_loss: 5.7887\n",
      "Epoch 17/20\n",
      "157/157 - 8s - 52ms/step - accuracy: 0.9352 - loss: 0.2409 - val_accuracy: 0.2684 - val_loss: 6.0918\n",
      "Epoch 18/20\n",
      "157/157 - 9s - 57ms/step - accuracy: 0.9515 - loss: 0.1803 - val_accuracy: 0.2588 - val_loss: 6.5547\n",
      "Epoch 19/20\n",
      "157/157 - 9s - 55ms/step - accuracy: 0.9439 - loss: 0.1960 - val_accuracy: 0.2580 - val_loss: 6.8342\n",
      "Epoch 20/20\n",
      "157/157 - 9s - 54ms/step - accuracy: 0.9698 - loss: 0.1158 - val_accuracy: 0.2519 - val_loss: 7.3359\n",
      "Supervisado: Train Accuracy: 0.9668 - Test Accuracy: 0.2519\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(100, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model_sup = build_model()\n",
    "\n",
    "\n",
    "# Entrenamiento con solo datos etiquetados\n",
    "history_sup = model_sup.fit(x_train_labeled, y_train_labeled,\n",
    "                            epochs=20, batch_size=64,\n",
    "                            validation_data=(x_test, y_test_cat),\n",
    "                            verbose=2)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "train_loss, train_acc = model_sup.evaluate(x_train_labeled, y_train_labeled, verbose=0)\n",
    "test_loss, test_acc   = model_sup.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"Supervisado: Train Accuracy: {:.4f} - Test Accuracy: {:.4f}\".format(train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios del Ejercicio 1\n",
    "\n",
    "- **Arquitectura:** Se utiliz√≥ una red CNN con dos bloques convolucionales y una capa densa final para clasificaci√≥n.\n",
    "- **Entrenamiento:** Se entren√≥ con 20 √©pocas usando Adam y una funci√≥n de p√©rdida de entrop√≠a cruzada.\n",
    "- **Resultados:** Se muestran las m√©tricas en entrenamiento y prueba. Probablemente se observe un rendimiento moderado debido a la cantidad reducida de datos etiquetados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio 2: Auto-aprendizaje (Self-Training)\n",
    "\n",
    "En este ejercicio se reutiliza el mismo modelo definido en el Ejercicio 1, pero se incorporan las instancias sin etiquetar mediante la t√©cnica de auto-aprendizaje. La idea es:\n",
    "1. Entrenar el modelo inicialmente con los datos etiquetados.\n",
    "2. Utilizar el modelo para predecir etiquetas en los datos sin etiquetar.\n",
    "3. Seleccionar aquellas predicciones con alta certeza (por ejemplo, una probabilidad mayor a un umbral definido) y agregarlas al conjunto de entrenamiento, ponderando (opcionalmente) su contribuci√≥n seg√∫n la confianza.\n",
    "\n",
    "### Preguntas a responder:\n",
    "a. **¬øQu√© par√°metros has definido para el entrenamiento?**  \n",
    "   Se define un umbral de confianza (por ejemplo, 0.9) para aceptar pseudo-etiquetas.\n",
    "\n",
    "b. **Rendimiento en entrenamiento y prueba.**\n",
    "\n",
    "c. **¬øSe mejoran los resultados respecto al Ejercicio 1?**\n",
    "\n",
    "d. **Conclusiones sobre los resultados.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step\n",
      "N√∫mero de pseudo-etiquetas seleccionadas: 16235\n",
      "Epoch 1/20\n",
      "410/410 - 22s - 53ms/step - accuracy: 0.1776 - loss: 3.6094 - val_accuracy: 0.1596 - val_loss: 3.6536\n",
      "Epoch 2/20\n",
      "410/410 - 19s - 47ms/step - accuracy: 0.3629 - loss: 2.5858 - val_accuracy: 0.2129 - val_loss: 3.4047\n",
      "Epoch 3/20\n",
      "410/410 - 19s - 47ms/step - accuracy: 0.4512 - loss: 2.1455 - val_accuracy: 0.2512 - val_loss: 3.2654\n",
      "Epoch 4/20\n",
      "410/410 - 19s - 46ms/step - accuracy: 0.5196 - loss: 1.8406 - val_accuracy: 0.2679 - val_loss: 3.3451\n",
      "Epoch 5/20\n",
      "410/410 - 21s - 51ms/step - accuracy: 0.5748 - loss: 1.5938 - val_accuracy: 0.2737 - val_loss: 3.3397\n",
      "Epoch 6/20\n",
      "410/410 - 21s - 50ms/step - accuracy: 0.6224 - loss: 1.3719 - val_accuracy: 0.2726 - val_loss: 3.5444\n",
      "Epoch 7/20\n",
      "410/410 - 18s - 43ms/step - accuracy: 0.6699 - loss: 1.1656 - val_accuracy: 0.2691 - val_loss: 3.7738\n",
      "Epoch 8/20\n",
      "410/410 - 19s - 46ms/step - accuracy: 0.7223 - loss: 0.9733 - val_accuracy: 0.2763 - val_loss: 3.9388\n",
      "Epoch 9/20\n",
      "410/410 - 18s - 44ms/step - accuracy: 0.7672 - loss: 0.7983 - val_accuracy: 0.2705 - val_loss: 4.2437\n",
      "Epoch 10/20\n",
      "410/410 - 18s - 44ms/step - accuracy: 0.8067 - loss: 0.6483 - val_accuracy: 0.2692 - val_loss: 4.8252\n",
      "Epoch 11/20\n",
      "410/410 - 18s - 45ms/step - accuracy: 0.8415 - loss: 0.5132 - val_accuracy: 0.2625 - val_loss: 5.2624\n",
      "Epoch 12/20\n",
      "410/410 - 17s - 43ms/step - accuracy: 0.8692 - loss: 0.4210 - val_accuracy: 0.2631 - val_loss: 5.7747\n",
      "Epoch 13/20\n",
      "410/410 - 19s - 46ms/step - accuracy: 0.8950 - loss: 0.3351 - val_accuracy: 0.2612 - val_loss: 6.2624\n",
      "Epoch 14/20\n",
      "410/410 - 19s - 47ms/step - accuracy: 0.9105 - loss: 0.2796 - val_accuracy: 0.2611 - val_loss: 6.7290\n",
      "Epoch 15/20\n",
      "410/410 - 18s - 44ms/step - accuracy: 0.9190 - loss: 0.2546 - val_accuracy: 0.2588 - val_loss: 6.9078\n",
      "Epoch 16/20\n",
      "410/410 - 18s - 45ms/step - accuracy: 0.9272 - loss: 0.2223 - val_accuracy: 0.2598 - val_loss: 7.4732\n",
      "Epoch 17/20\n",
      "410/410 - 18s - 43ms/step - accuracy: 0.9314 - loss: 0.2106 - val_accuracy: 0.2552 - val_loss: 7.6942\n",
      "Epoch 18/20\n",
      "410/410 - 19s - 47ms/step - accuracy: 0.9444 - loss: 0.1762 - val_accuracy: 0.2585 - val_loss: 8.2000\n",
      "Epoch 19/20\n",
      "410/410 - 18s - 44ms/step - accuracy: 0.9432 - loss: 0.1722 - val_accuracy: 0.2561 - val_loss: 8.4686\n",
      "Epoch 20/20\n",
      "410/410 - 18s - 44ms/step - accuracy: 0.9425 - loss: 0.1735 - val_accuracy: 0.2541 - val_loss: 7.9876\n",
      "Self-training: Train Accuracy: 0.9508 - Test Accuracy: 0.2541\n"
     ]
    }
   ],
   "source": [
    "# Umbral para la certeza de las pseudo-etiquetas\n",
    "confidence_threshold = 0.9\n",
    "\n",
    "# Realizar predicciones en los datos sin etiquetar\n",
    "pseudo_labels_prob = model_sup.predict(x_train_unlabeled)\n",
    "pseudo_labels = np.argmax(pseudo_labels_prob, axis=1)\n",
    "pseudo_labels_cat = to_categorical(pseudo_labels, 100)\n",
    "\n",
    "# Seleccionar aquellos ejemplos con alta confianza\n",
    "max_probs = np.max(pseudo_labels_prob, axis=1)\n",
    "selected = max_probs >= confidence_threshold\n",
    "\n",
    "print(\"N√∫mero de pseudo-etiquetas seleccionadas:\", np.sum(selected))\n",
    "\n",
    "# Combinar datos etiquetados con pseudo-etiquetados de alta confianza\n",
    "x_combined = np.concatenate([x_train_labeled, x_train_unlabeled[selected]], axis=0)\n",
    "y_combined = np.concatenate([y_train_labeled, pseudo_labels_cat[selected]], axis=0)\n",
    "\n",
    "# Reiniciar el modelo (o seguir afinando)\n",
    "model_self = build_model()\n",
    "\n",
    "history_self = model_self.fit(x_combined, y_combined,\n",
    "                              epochs=20, batch_size=64,\n",
    "                              validation_data=(x_test, y_test_cat),\n",
    "                              verbose=2)\n",
    "\n",
    "train_loss_self, train_acc_self = model_self.evaluate(x_combined, y_combined, verbose=0)\n",
    "test_loss_self, test_acc_self   = model_self.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"Self-training: Train Accuracy: {:.4f} - Test Accuracy: {:.4f}\".format(train_acc_self, test_acc_self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios del Ejercicio 2\n",
    "\n",
    "- **Par√°metros:** Se ha definido un umbral de confianza de 0.9 para la selecci√≥n de pseudo-etiquetas.\n",
    "- **Resultados:** Se eval√∫a el rendimiento en entrenamiento y prueba del modelo entrenado con datos combinados.  \n",
    "- **Comparaci√≥n:** Se discute si el auto-aprendizaje mejora o no los resultados obtenidos en el Ejercicio 1, teniendo en cuenta que se incorpora informaci√≥n extra con cierto nivel de incertidumbre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Ejercicio 3: Autoencoder en dos pasos\n",
    "\n",
    "Este ejercicio se realiza en dos etapas:\n",
    "1. **Entrenamiento del autoencoder:** Se entrena un autoencoder que utiliza como encoder la parte convolucional (igual que la definida en los ejercicios anteriores, salvo el √∫ltimo bloque) para aprender una representaci√≥n compacta de las im√°genes.\n",
    "2. **Entrenamiento del clasificador:** Se utiliza el encoder preentrenado, se congela y se a√±ade una cabeza de clasificaci√≥n (capas densas) que se entrena con los datos etiquetados.\n",
    "\n",
    "### Preguntas a responder:\n",
    "a. **Arquitectura y hiperpar√°metros.**\n",
    "\n",
    "b. **Rendimiento en entrenamiento y prueba.**\n",
    "\n",
    "c. **Comparaci√≥n con los Ejercicios 1 y 2.**\n",
    "\n",
    "d. **Conclusiones.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 - 50s - 129ms/step - loss: 0.0113 - val_loss: 0.0059\n",
      "Epoch 2/20\n",
      "391/391 - 43s - 109ms/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 3/20\n",
      "391/391 - 42s - 107ms/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 4/20\n",
      "391/391 - 41s - 105ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 5/20\n",
      "391/391 - 42s - 108ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 6/20\n",
      "391/391 - 42s - 107ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 7/20\n",
      "391/391 - 43s - 110ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 8/20\n",
      "391/391 - 50s - 128ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 9/20\n",
      "391/391 - 43s - 109ms/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 10/20\n",
      "391/391 - 40s - 103ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 11/20\n",
      "391/391 - 43s - 109ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 12/20\n",
      "391/391 - 45s - 116ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 13/20\n",
      "391/391 - 50s - 127ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 14/20\n",
      "391/391 - 49s - 125ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 15/20\n",
      "391/391 - 45s - 115ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 16/20\n",
      "391/391 - 49s - 125ms/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 17/20\n",
      "391/391 - 50s - 128ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 18/20\n",
      "391/391 - 55s - 141ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 19/20\n",
      "391/391 - 52s - 133ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 20/20\n",
      "391/391 - 51s - 130ms/step - loss: 0.0023 - val_loss: 0.0024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a7d8a47740>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir el autoencoder\n",
    "def build_autoencoder():\n",
    "    input_img = layers.Input(shape=x_train.shape[1:])\n",
    "    # Encoder: se reutiliza parte de la arquitectura de Ej.1\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    encoded = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "    \n",
    "    # Decoder: arquitectura sim√©trica\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(encoded)\n",
    "    x = layers.UpSampling2D((2,2))(x)\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2,2))(x)\n",
    "    decoded = layers.Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = models.Model(input_img, decoded)\n",
    "    encoder = models.Model(input_img, encoded)\n",
    "    return autoencoder, encoder\n",
    "\n",
    "autoencoder, encoder = build_autoencoder()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entrenar el autoencoder con todas las im√°genes de entrenamiento (se puede usar tanto etiquetadas como sin etiquetar)\n",
    "x_autoencoder = x_train  # Uso completo del entrenamiento sin distinguir etiqueta\n",
    "autoencoder.fit(x_autoencoder, x_autoencoder,\n",
    "                epochs=20, batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 - 11s - 68ms/step - accuracy: 0.0288 - loss: 4.5171 - val_accuracy: 0.0401 - val_loss: 4.3398\n",
      "Epoch 2/20\n",
      "157/157 - 6s - 38ms/step - accuracy: 0.0691 - loss: 4.1482 - val_accuracy: 0.0769 - val_loss: 4.0751\n",
      "Epoch 3/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.1034 - loss: 3.9153 - val_accuracy: 0.1037 - val_loss: 3.9084\n",
      "Epoch 4/20\n",
      "157/157 - 5s - 31ms/step - accuracy: 0.1301 - loss: 3.7670 - val_accuracy: 0.1252 - val_loss: 3.7961\n",
      "Epoch 5/20\n",
      "157/157 - 5s - 31ms/step - accuracy: 0.1469 - loss: 3.6550 - val_accuracy: 0.1340 - val_loss: 3.7330\n",
      "Epoch 6/20\n",
      "157/157 - 5s - 33ms/step - accuracy: 0.1591 - loss: 3.5660 - val_accuracy: 0.1434 - val_loss: 3.6539\n",
      "Epoch 7/20\n",
      "157/157 - 5s - 32ms/step - accuracy: 0.1724 - loss: 3.4906 - val_accuracy: 0.1541 - val_loss: 3.6315\n",
      "Epoch 8/20\n",
      "157/157 - 5s - 29ms/step - accuracy: 0.1894 - loss: 3.4249 - val_accuracy: 0.1645 - val_loss: 3.5773\n",
      "Epoch 9/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.1982 - loss: 3.3687 - val_accuracy: 0.1706 - val_loss: 3.5220\n",
      "Epoch 10/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2037 - loss: 3.3169 - val_accuracy: 0.1728 - val_loss: 3.4985\n",
      "Epoch 11/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2125 - loss: 3.2831 - val_accuracy: 0.1784 - val_loss: 3.4761\n",
      "Epoch 12/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2202 - loss: 3.2331 - val_accuracy: 0.1915 - val_loss: 3.4256\n",
      "Epoch 13/20\n",
      "157/157 - 5s - 33ms/step - accuracy: 0.2306 - loss: 3.1949 - val_accuracy: 0.1932 - val_loss: 3.4024\n",
      "Epoch 14/20\n",
      "157/157 - 5s - 32ms/step - accuracy: 0.2378 - loss: 3.1582 - val_accuracy: 0.1940 - val_loss: 3.4180\n",
      "Epoch 15/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2458 - loss: 3.1251 - val_accuracy: 0.2028 - val_loss: 3.3761\n",
      "Epoch 16/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2475 - loss: 3.0918 - val_accuracy: 0.1956 - val_loss: 3.3922\n",
      "Epoch 17/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2593 - loss: 3.0566 - val_accuracy: 0.2044 - val_loss: 3.3517\n",
      "Epoch 18/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2579 - loss: 3.0381 - val_accuracy: 0.2019 - val_loss: 3.3691\n",
      "Epoch 19/20\n",
      "157/157 - 4s - 28ms/step - accuracy: 0.2695 - loss: 3.0032 - val_accuracy: 0.2113 - val_loss: 3.3218\n",
      "Epoch 20/20\n",
      "157/157 - 5s - 29ms/step - accuracy: 0.2757 - loss: 2.9722 - val_accuracy: 0.2132 - val_loss: 3.3222\n",
      "Autoencoder (2 pasos): Train Accuracy: 0.2861 - Test Accuracy: 0.2132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% \n",
    "# Ahora, se congela el encoder y se a√±ade una cabeza de clasificaci√≥n\n",
    "for layer in encoder.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construir el clasificador utilizando el encoder\n",
    "encoded_input = layers.Input(shape=encoder.output_shape[1:])\n",
    "x = layers.Flatten()(encoded_input)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "output = layers.Dense(100, activation='softmax')(x)\n",
    "classifier_head = models.Model(encoded_input, output)\n",
    "\n",
    "# Conectar el encoder y la cabeza de clasificaci√≥n\n",
    "input_img = layers.Input(shape=x_train.shape[1:])\n",
    "features = encoder(input_img)\n",
    "classifier_output = classifier_head(features)\n",
    "model_autoencoder_classifier = models.Model(input_img, classifier_output)\n",
    "\n",
    "model_autoencoder_classifier.compile(optimizer=optimizers.Adam(),\n",
    "                                     loss='categorical_crossentropy',\n",
    "                                     metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el clasificador con los datos etiquetados\n",
    "history_ae = model_autoencoder_classifier.fit(x_train_labeled, y_train_labeled,\n",
    "                                              epochs=20, batch_size=64,\n",
    "                                              validation_data=(x_test, y_test_cat),\n",
    "                                              verbose=2)\n",
    "\n",
    "train_loss_ae, train_acc_ae = model_autoencoder_classifier.evaluate(x_train_labeled, y_train_labeled, verbose=0)\n",
    "test_loss_ae, test_acc_ae   = model_autoencoder_classifier.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"Autoencoder (2 pasos): Train Accuracy: {:.4f} - Test Accuracy: {:.4f}\".format(train_acc_ae, test_acc_ae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comentarios del Ejercicio 3\n",
    "\n",
    "- **Arquitectura:**  \n",
    "  - *Autoencoder:* Se entrena un modelo que aprende a reconstruir las im√°genes.  \n",
    "  - *Encoder:* Es la parte convolucional preentrenada.  \n",
    "  - *Clasificador:* Se a√±ade una cabeza densa para la clasificaci√≥n.\n",
    "- **Hiperpar√°metros:** Se emple√≥ MSE para la reconstrucci√≥n y entrop√≠a cruzada para la clasificaci√≥n, con 20 √©pocas en cada fase.\n",
    "- **Resultados:** Se eval√∫a el rendimiento en entrenamiento y prueba, y se compara con los enfoques anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio 4: Autoencoder con clasificaci√≥n simult√°nea (una etapa)\n",
    "\n",
    "En este ejercicio se entrena un modelo que combina la reconstrucci√≥n del autoencoder y la clasificaci√≥n en un √∫nico entrenamiento. La arquitectura del encoder es la misma que en el Ejercicio 3 y la combinaci√≥n encoder+clasificador es similar al del Ejercicio 1.\n",
    "\n",
    "### Preguntas a responder:\n",
    "a. **Arquitectura y hiperpar√°metros.**\n",
    "\n",
    "b. **Rendimiento en entrenamiento y prueba.**\n",
    "\n",
    "c. **¬øSe mejoran los resultados?**\n",
    "\n",
    "d. **Conclusiones.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 - 18s - 116ms/step - classification_accuracy: 0.0714 - classification_loss: 4.1732 - decoded_loss: 0.0285 - loss: 4.1905 - val_classification_accuracy: 0.1137 - val_classification_loss: 3.8598 - val_decoded_loss: 0.0173 - val_loss: 3.8683\n",
      "Epoch 2/20\n",
      "157/157 - 16s - 99ms/step - classification_accuracy: 0.1807 - classification_loss: 3.4942 - decoded_loss: 0.0126 - loss: 3.5025 - val_classification_accuracy: 0.1929 - val_classification_loss: 3.4173 - val_decoded_loss: 0.0113 - val_loss: 3.4211\n",
      "Epoch 3/20\n",
      "157/157 - 15s - 95ms/step - classification_accuracy: 0.2623 - classification_loss: 3.0445 - decoded_loss: 0.0105 - loss: 3.0469 - val_classification_accuracy: 0.2231 - val_classification_loss: 3.2541 - val_decoded_loss: 0.0138 - val_loss: 3.2597\n",
      "Epoch 4/20\n",
      "157/157 - 19s - 122ms/step - classification_accuracy: 0.3337 - classification_loss: 2.6512 - decoded_loss: 0.0094 - loss: 2.6523 - val_classification_accuracy: 0.2429 - val_classification_loss: 3.1550 - val_decoded_loss: 0.0085 - val_loss: 3.1581\n",
      "Epoch 5/20\n",
      "157/157 - 19s - 123ms/step - classification_accuracy: 0.4135 - classification_loss: 2.2585 - decoded_loss: 0.0084 - loss: 2.2630 - val_classification_accuracy: 0.2569 - val_classification_loss: 3.1575 - val_decoded_loss: 0.0080 - val_loss: 3.1608\n",
      "Epoch 6/20\n",
      "157/157 - 17s - 109ms/step - classification_accuracy: 0.5009 - classification_loss: 1.8905 - decoded_loss: 0.0079 - loss: 1.8915 - val_classification_accuracy: 0.2627 - val_classification_loss: 3.1958 - val_decoded_loss: 0.0076 - val_loss: 3.1996\n",
      "Epoch 7/20\n",
      "157/157 - 17s - 107ms/step - classification_accuracy: 0.5962 - classification_loss: 1.5122 - decoded_loss: 0.0078 - loss: 1.5128 - val_classification_accuracy: 0.2668 - val_classification_loss: 3.3781 - val_decoded_loss: 0.0071 - val_loss: 3.3823\n",
      "Epoch 8/20\n",
      "157/157 - 17s - 107ms/step - classification_accuracy: 0.6931 - classification_loss: 1.1297 - decoded_loss: 0.0074 - loss: 1.1338 - val_classification_accuracy: 0.2706 - val_classification_loss: 3.5600 - val_decoded_loss: 0.0068 - val_loss: 3.5634\n",
      "Epoch 9/20\n",
      "157/157 - 20s - 129ms/step - classification_accuracy: 0.7843 - classification_loss: 0.8083 - decoded_loss: 0.0068 - loss: 0.8119 - val_classification_accuracy: 0.2729 - val_classification_loss: 3.8859 - val_decoded_loss: 0.0065 - val_loss: 3.8897\n",
      "Epoch 10/20\n",
      "157/157 - 21s - 131ms/step - classification_accuracy: 0.8618 - classification_loss: 0.5301 - decoded_loss: 0.0064 - loss: 0.5346 - val_classification_accuracy: 0.2702 - val_classification_loss: 4.2497 - val_decoded_loss: 0.0061 - val_loss: 4.2552\n",
      "Epoch 11/20\n",
      "157/157 - 18s - 115ms/step - classification_accuracy: 0.9141 - classification_loss: 0.3464 - decoded_loss: 0.0060 - loss: 0.3496 - val_classification_accuracy: 0.2662 - val_classification_loss: 4.7275 - val_decoded_loss: 0.0059 - val_loss: 4.7313\n",
      "Epoch 12/20\n",
      "157/157 - 16s - 100ms/step - classification_accuracy: 0.9510 - classification_loss: 0.2224 - decoded_loss: 0.0059 - loss: 0.2254 - val_classification_accuracy: 0.2640 - val_classification_loss: 5.0157 - val_decoded_loss: 0.0065 - val_loss: 5.0233\n",
      "Epoch 13/20\n",
      "157/157 - 15s - 95ms/step - classification_accuracy: 0.9723 - classification_loss: 0.1344 - decoded_loss: 0.0057 - loss: 0.1375 - val_classification_accuracy: 0.2645 - val_classification_loss: 5.3541 - val_decoded_loss: 0.0056 - val_loss: 5.3603\n",
      "Epoch 14/20\n",
      "157/157 - 15s - 96ms/step - classification_accuracy: 0.9803 - classification_loss: 0.0988 - decoded_loss: 0.0052 - loss: 0.1015 - val_classification_accuracy: 0.2588 - val_classification_loss: 5.7906 - val_decoded_loss: 0.0052 - val_loss: 5.7935\n",
      "Epoch 15/20\n",
      "157/157 - 15s - 93ms/step - classification_accuracy: 0.9820 - classification_loss: 0.0866 - decoded_loss: 0.0051 - loss: 0.0890 - val_classification_accuracy: 0.2615 - val_classification_loss: 5.8974 - val_decoded_loss: 0.0055 - val_loss: 5.9070\n",
      "Epoch 16/20\n",
      "157/157 - 15s - 95ms/step - classification_accuracy: 0.9807 - classification_loss: 0.0825 - decoded_loss: 0.0051 - loss: 0.0852 - val_classification_accuracy: 0.2544 - val_classification_loss: 5.9307 - val_decoded_loss: 0.0056 - val_loss: 5.9400\n",
      "Epoch 17/20\n",
      "157/157 - 15s - 97ms/step - classification_accuracy: 0.9685 - classification_loss: 0.1194 - decoded_loss: 0.0052 - loss: 0.1211 - val_classification_accuracy: 0.2527 - val_classification_loss: 6.2473 - val_decoded_loss: 0.0054 - val_loss: 6.2502\n",
      "Epoch 18/20\n",
      "157/157 - 15s - 94ms/step - classification_accuracy: 0.9583 - classification_loss: 0.1536 - decoded_loss: 0.0053 - loss: 0.1550 - val_classification_accuracy: 0.2498 - val_classification_loss: 6.0578 - val_decoded_loss: 0.0059 - val_loss: 6.0644\n",
      "Epoch 19/20\n",
      "157/157 - 15s - 93ms/step - classification_accuracy: 0.9714 - classification_loss: 0.1118 - decoded_loss: 0.0054 - loss: 0.1150 - val_classification_accuracy: 0.2644 - val_classification_loss: 6.4527 - val_decoded_loss: 0.0050 - val_loss: 6.4596\n",
      "Epoch 20/20\n",
      "157/157 - 15s - 95ms/step - classification_accuracy: 0.9887 - classification_loss: 0.0531 - decoded_loss: 0.0048 - loss: 0.0556 - val_classification_accuracy: 0.2595 - val_classification_loss: 6.7004 - val_decoded_loss: 0.0048 - val_loss: 6.7074\n",
      "Multi-tarea: Test Accuracy (clasificaci√≥n): 0.2595\n"
     ]
    }
   ],
   "source": [
    "# Definir un modelo multitarea: reconstrucci√≥n y clasificaci√≥n\n",
    "input_img = layers.Input(shape=x_train.shape[1:])\n",
    "\n",
    "# Encoder (mismo que antes)\n",
    "x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "encoded = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "\n",
    "# Rama de reconstrucci√≥n (decoder)\n",
    "x_rec = layers.Conv2D(64, (3,3), activation='relu', padding='same')(encoded)\n",
    "x_rec = layers.UpSampling2D((2,2))(x_rec)\n",
    "x_rec = layers.Conv2D(32, (3,3), activation='relu', padding='same')(x_rec)\n",
    "x_rec = layers.UpSampling2D((2,2))(x_rec)\n",
    "decoded = layers.Conv2D(3, (3,3), activation='sigmoid', padding='same', name='decoded')(x_rec)\n",
    "\n",
    "# Rama de clasificaci√≥n (cabeza)\n",
    "x_cls = layers.Flatten()(encoded)\n",
    "x_cls = layers.Dense(512, activation='relu')(x_cls)\n",
    "classification = layers.Dense(100, activation='softmax', name='classification')(x_cls)\n",
    "\n",
    "model_multi = models.Model(input_img, [decoded, classification])\n",
    "\n",
    "# Compilaci√≥n con dos p√©rdidas: reconstrucci√≥n (MSE) y clasificaci√≥n (entrop√≠a cruzada).\n",
    "model_multi.compile(optimizer='adam',\n",
    "                    loss={'decoded': 'mse', 'classification': 'categorical_crossentropy'},\n",
    "                    loss_weights={'decoded': 0.5, 'classification': 1.0},\n",
    "                    metrics={'classification': 'accuracy'})\n",
    "\n",
    "# Entrenar el modelo multitarea: para la rama de reconstrucci√≥n se usan las im√°genes de entrada\n",
    "history_multi = model_multi.fit(x_train_labeled, {'decoded': x_train_labeled, 'classification': y_train_labeled},\n",
    "                                epochs=20, batch_size=64,\n",
    "                                validation_data=(x_test, {'decoded': x_test, 'classification': y_test_cat}),\n",
    "                                verbose=2)\n",
    "\n",
    "# Evaluar solo la parte clasificadora\n",
    "results = model_multi.evaluate(x_test, {'decoded': x_test, 'classification': y_test_cat}, verbose=0)\n",
    "print(\"Multi-tarea: Test Accuracy (clasificaci√≥n): {:.4f}\".format(results[3]))  # results[3] corresponde a la m√©trica de 'classification'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comentarios del Ejercicio 4\n",
    "\n",
    "- **Arquitectura:** Se utiliza un modelo con dos salidas, una para la reconstrucci√≥n de la imagen y otra para la clasificaci√≥n.\n",
    "- **Hiperpar√°metros:** Se emplean dos p√©rdidas (ponderadas) y se entrena en 20 √©pocas.\n",
    "- **Resultados:** Se analizan las m√©tricas de clasificaci√≥n y se comparan con los ejercicios anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Ejercicio 5: Eliminaci√≥n de instancias no etiquetadas at√≠picas\n",
    "\n",
    "En este ejercicio se repiten los entrenamientos de los Ejercicios 1‚Äì4 pero se eliminan las instancias sin etiquetar consideradas at√≠picas en relaci√≥n con los datos etiquetados. Se utiliza la t√©cnica explicada (por ejemplo, seleccionando solo aquellas pseudo-etiquetas cuya confianza sea superior a ùë£ = 0.9) y se utiliza la misma arquitectura de clasificaci√≥n que en el Ejercicio 1, salvo la capa de salida.\n",
    "\n",
    "### Pregunta a responder:\n",
    "a. ¬øSe mejoran los resultados con respecto a los ejercicios anteriores? ¬øQu√© conclusiones se sacan?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias sin etiqueta seleccionadas (v=0.9): 16235\n",
      "Epoch 1/20\n",
      "410/410 - 20s - 48ms/step - accuracy: 0.1838 - loss: 3.5806 - val_accuracy: 0.1782 - val_loss: 3.6387\n",
      "Epoch 2/20\n",
      "410/410 - 20s - 49ms/step - accuracy: 0.3685 - loss: 2.5608 - val_accuracy: 0.2220 - val_loss: 3.3681\n",
      "Epoch 3/20\n",
      "410/410 - 21s - 51ms/step - accuracy: 0.4492 - loss: 2.1403 - val_accuracy: 0.2571 - val_loss: 3.2083\n",
      "Epoch 4/20\n",
      "410/410 - 21s - 51ms/step - accuracy: 0.5180 - loss: 1.8396 - val_accuracy: 0.2543 - val_loss: 3.4349\n",
      "Epoch 5/20\n",
      "410/410 - 19s - 47ms/step - accuracy: 0.5706 - loss: 1.5899 - val_accuracy: 0.2771 - val_loss: 3.3939\n",
      "Epoch 6/20\n",
      "410/410 - 18s - 43ms/step - accuracy: 0.6202 - loss: 1.3802 - val_accuracy: 0.2770 - val_loss: 3.5677\n",
      "Epoch 7/20\n",
      "410/410 - 18s - 44ms/step - accuracy: 0.6705 - loss: 1.1765 - val_accuracy: 0.2682 - val_loss: 3.7762\n",
      "Epoch 8/20\n",
      "410/410 - 20s - 49ms/step - accuracy: 0.7126 - loss: 0.9936 - val_accuracy: 0.2686 - val_loss: 3.9810\n",
      "Epoch 9/20\n",
      "410/410 - 21s - 51ms/step - accuracy: 0.7599 - loss: 0.8246 - val_accuracy: 0.2619 - val_loss: 4.1719\n",
      "Epoch 10/20\n",
      "410/410 - 20s - 49ms/step - accuracy: 0.8025 - loss: 0.6669 - val_accuracy: 0.2656 - val_loss: 4.6520\n",
      "Epoch 11/20\n",
      "410/410 - 20s - 49ms/step - accuracy: 0.8415 - loss: 0.5306 - val_accuracy: 0.2687 - val_loss: 5.1049\n",
      "Epoch 12/20\n",
      "410/410 - 20s - 50ms/step - accuracy: 0.8688 - loss: 0.4286 - val_accuracy: 0.2613 - val_loss: 5.4995\n",
      "Epoch 13/20\n",
      "410/410 - 20s - 49ms/step - accuracy: 0.8899 - loss: 0.3478 - val_accuracy: 0.2630 - val_loss: 6.0859\n",
      "Epoch 14/20\n",
      "410/410 - 21s - 51ms/step - accuracy: 0.9010 - loss: 0.3057 - val_accuracy: 0.2607 - val_loss: 6.4810\n",
      "Epoch 15/20\n",
      "410/410 - 20s - 50ms/step - accuracy: 0.9226 - loss: 0.2481 - val_accuracy: 0.2631 - val_loss: 6.9413\n",
      "Epoch 16/20\n",
      "410/410 - 21s - 51ms/step - accuracy: 0.9322 - loss: 0.2145 - val_accuracy: 0.2535 - val_loss: 7.6682\n",
      "Epoch 17/20\n",
      "410/410 - 21s - 52ms/step - accuracy: 0.9345 - loss: 0.2050 - val_accuracy: 0.2566 - val_loss: 7.6571\n",
      "Epoch 18/20\n",
      "410/410 - 21s - 51ms/step - accuracy: 0.9328 - loss: 0.2024 - val_accuracy: 0.2512 - val_loss: 8.0134\n",
      "Epoch 19/20\n",
      "410/410 - 20s - 50ms/step - accuracy: 0.9383 - loss: 0.1853 - val_accuracy: 0.2601 - val_loss: 8.4433\n",
      "Epoch 20/20\n",
      "410/410 - 20s - 50ms/step - accuracy: 0.9475 - loss: 0.1624 - val_accuracy: 0.2559 - val_loss: 8.6171\n",
      "Filtered Self-training: Train Accuracy: 0.9551 - Test Accuracy: 0.2559\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: Reaplicar el auto-aprendizaje del Ejercicio 2 con selecci√≥n basada en v=0.9\n",
    "v = 0.9  # umbral definido\n",
    "max_probs = np.max(pseudo_labels_prob, axis=1)\n",
    "selected_v = max_probs >= v\n",
    "print(\"Instancias sin etiqueta seleccionadas (v=0.9):\", np.sum(selected_v))\n",
    "\n",
    "# Se combinan los datos etiquetados con los pseudo-etiquetados filtrados\n",
    "x_combined_v = np.concatenate([x_train_labeled, x_train_unlabeled[selected_v]], axis=0)\n",
    "y_combined_v = np.concatenate([y_train_labeled, pseudo_labels_cat[selected_v]], axis=0)\n",
    "\n",
    "# Reiniciar y entrenar el modelo supervisado (con arquitectura similar a Ej.1)\n",
    "model_filtered = build_model()\n",
    "model_filtered.compile(optimizer=optimizers.Adam(),\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "history_filtered = model_filtered.fit(x_combined_v, y_combined_v,\n",
    "                                      epochs=20, batch_size=64,\n",
    "                                      validation_data=(x_test, y_test_cat),\n",
    "                                      verbose=2)\n",
    "\n",
    "train_loss_f, train_acc_f = model_filtered.evaluate(x_combined_v, y_combined_v, verbose=0)\n",
    "test_loss_f, test_acc_f   = model_filtered.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"Filtered Self-training: Train Accuracy: {:.4f} - Test Accuracy: {:.4f}\".format(train_acc_f, test_acc_f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comentarios del Ejercicio 5\n",
    "\n",
    "- Se ha aplicado la t√©cnica de filtrado utilizando un umbral ùë£ = 0.9 para eliminar los ejemplos sin etiqueta menos confiables.\n",
    "- Se compara el rendimiento del modelo con el obtenido en ejercicios anteriores.\n",
    "- Se discuten las mejoras y las limitaciones encontradas tras eliminar los datos at√≠picos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Ejercicio 6: Uso de la t√©cnica \"Hay vida m√°s all√° del autoencoder\"\n",
    "\n",
    "En este ejercicio se repiten los entrenamientos de los Ejercicios 3‚Äì5 cambiando el autoencoder por la t√©cnica definida en el apartado ‚ÄúHay vida m√°s all√° del autoencoder‚Äù. La arquitectura de la red se mantiene igual que la parte encoder del autoencoder definido anteriormente, y se asegura que el modelo entrene correctamente.\n",
    "\n",
    "### Preguntas a responder:\n",
    "a. Arquitectura y hiperpar√°metros.\n",
    "b. Rendimiento en entrenamiento y prueba.\n",
    "c. Comparaci√≥n con los ejercicios anteriores.\n",
    "d. Conclusiones.\n",
    " \n",
    "**Nota:** La implementaci√≥n de esta t√©cnica puede variar; a continuaci√≥n se muestra una posible aproximaci√≥n en la que se modifica el esquema de entrenamiento para obtener representaciones √∫tiles para la clasificaci√≥n sin reconstruir la imagen completa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 - 11s - 68ms/step - accuracy: 0.0568 - loss: 4.2492 - val_accuracy: 0.1048 - val_loss: 3.9059\n",
      "Epoch 2/20\n",
      "157/157 - 9s - 56ms/step - accuracy: 0.1700 - loss: 3.5422 - val_accuracy: 0.1690 - val_loss: 3.5670\n",
      "Epoch 3/20\n",
      "157/157 - 11s - 73ms/step - accuracy: 0.2479 - loss: 3.0884 - val_accuracy: 0.2249 - val_loss: 3.2653\n",
      "Epoch 4/20\n",
      "157/157 - 12s - 77ms/step - accuracy: 0.3212 - loss: 2.7126 - val_accuracy: 0.2292 - val_loss: 3.2572\n",
      "Epoch 5/20\n",
      "157/157 - 11s - 68ms/step - accuracy: 0.3996 - loss: 2.3408 - val_accuracy: 0.2610 - val_loss: 3.1362\n",
      "Epoch 6/20\n",
      "157/157 - 20s - 125ms/step - accuracy: 0.4890 - loss: 1.9524 - val_accuracy: 0.2688 - val_loss: 3.2375\n",
      "Epoch 7/20\n",
      "157/157 - 11s - 67ms/step - accuracy: 0.5841 - loss: 1.5567 - val_accuracy: 0.2696 - val_loss: 3.4451\n",
      "Epoch 8/20\n",
      "157/157 - 19s - 121ms/step - accuracy: 0.6810 - loss: 1.1898 - val_accuracy: 0.2629 - val_loss: 3.6283\n",
      "Epoch 9/20\n",
      "157/157 - 9s - 57ms/step - accuracy: 0.7762 - loss: 0.8519 - val_accuracy: 0.2637 - val_loss: 3.8695\n",
      "Epoch 10/20\n",
      "157/157 - 9s - 57ms/step - accuracy: 0.8477 - loss: 0.5866 - val_accuracy: 0.2608 - val_loss: 4.3849\n",
      "Epoch 11/20\n",
      "157/157 - 9s - 57ms/step - accuracy: 0.9011 - loss: 0.3885 - val_accuracy: 0.2600 - val_loss: 4.6717\n",
      "Epoch 12/20\n",
      "157/157 - 9s - 58ms/step - accuracy: 0.9370 - loss: 0.2663 - val_accuracy: 0.2630 - val_loss: 5.0776\n",
      "Epoch 13/20\n",
      "157/157 - 9s - 59ms/step - accuracy: 0.9588 - loss: 0.1785 - val_accuracy: 0.2595 - val_loss: 5.4999\n",
      "Epoch 14/20\n",
      "157/157 - 9s - 59ms/step - accuracy: 0.9731 - loss: 0.1287 - val_accuracy: 0.2545 - val_loss: 5.9892\n",
      "Epoch 15/20\n",
      "157/157 - 9s - 59ms/step - accuracy: 0.9785 - loss: 0.1015 - val_accuracy: 0.2612 - val_loss: 6.0721\n",
      "Epoch 16/20\n",
      "157/157 - 9s - 58ms/step - accuracy: 0.9800 - loss: 0.0918 - val_accuracy: 0.2576 - val_loss: 6.3267\n",
      "Epoch 17/20\n",
      "157/157 - 9s - 59ms/step - accuracy: 0.9792 - loss: 0.0872 - val_accuracy: 0.2490 - val_loss: 6.5786\n",
      "Epoch 18/20\n",
      "157/157 - 9s - 59ms/step - accuracy: 0.9726 - loss: 0.1105 - val_accuracy: 0.2583 - val_loss: 6.5523\n",
      "Epoch 19/20\n",
      "157/157 - 9s - 58ms/step - accuracy: 0.9644 - loss: 0.1340 - val_accuracy: 0.2476 - val_loss: 6.9149\n",
      "Epoch 20/20\n",
      "157/157 - 9s - 58ms/step - accuracy: 0.9628 - loss: 0.1531 - val_accuracy: 0.2567 - val_loss: 6.7059\n",
      "Epoch 1/10\n",
      "254/254 - 14s - 56ms/step - accuracy: 0.4643 - loss: 2.2108 - val_accuracy: 0.2527 - val_loss: 3.5134\n",
      "Epoch 2/10\n",
      "254/254 - 14s - 56ms/step - accuracy: 0.6494 - loss: 1.2412 - val_accuracy: 0.2587 - val_loss: 3.9441\n",
      "Epoch 3/10\n",
      "254/254 - 13s - 52ms/step - accuracy: 0.7740 - loss: 0.7769 - val_accuracy: 0.2521 - val_loss: 4.2050\n",
      "Epoch 4/10\n",
      "254/254 - 13s - 53ms/step - accuracy: 0.8702 - loss: 0.4436 - val_accuracy: 0.2483 - val_loss: 5.0190\n",
      "Epoch 5/10\n",
      "254/254 - 13s - 52ms/step - accuracy: 0.9415 - loss: 0.2181 - val_accuracy: 0.2533 - val_loss: 5.6625\n",
      "Epoch 6/10\n",
      "254/254 - 14s - 53ms/step - accuracy: 0.9731 - loss: 0.1112 - val_accuracy: 0.2516 - val_loss: 6.3959\n",
      "Epoch 7/10\n",
      "254/254 - 13s - 53ms/step - accuracy: 0.9869 - loss: 0.0588 - val_accuracy: 0.2551 - val_loss: 7.2030\n",
      "Epoch 8/10\n",
      "254/254 - 14s - 53ms/step - accuracy: 0.9949 - loss: 0.0283 - val_accuracy: 0.2465 - val_loss: 7.7166\n",
      "Epoch 9/10\n",
      "254/254 - 14s - 55ms/step - accuracy: 0.9815 - loss: 0.0717 - val_accuracy: 0.2414 - val_loss: 7.3326\n",
      "Epoch 10/10\n",
      "254/254 - 14s - 53ms/step - accuracy: 0.9740 - loss: 0.0894 - val_accuracy: 0.2503 - val_loss: 7.4765\n",
      "Alternative Model: Train Accuracy: 0.5820 - Test Accuracy: 0.2503\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: Construcci√≥n de un modelo basado en la t√©cnica alternativa\n",
    "def build_alternative_model():\n",
    "    input_img = layers.Input(shape=x_train.shape[1:])\n",
    "    # Encoder id√©ntico al anterior\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    encoded = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "    # Se entrena directamente para clasificaci√≥n\n",
    "    x_cls = layers.Flatten()(encoded)\n",
    "    x_cls = layers.Dense(512, activation='relu')(x_cls)\n",
    "    classification = layers.Dense(100, activation='softmax')(x_cls)\n",
    "    model_alt = models.Model(input_img, classification)\n",
    "    return model_alt\n",
    "\n",
    "# Entrenar el modelo alternativo utilizando la t√©cnica de filtrado (como en el Ej.5)\n",
    "model_alt = build_alternative_model()\n",
    "model_alt.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Se puede entrenar primero con los datos etiquetados y posteriormente incorporar los pseudo-datos filtrados\n",
    "history_alt_1 = model_alt.fit(x_train_labeled, y_train_labeled,\n",
    "                              epochs=20, batch_size=64,\n",
    "                              validation_data=(x_test, y_test_cat),\n",
    "                              verbose=2)\n",
    "\n",
    "# Incorporar datos pseudo-etiquetados (filtrados con el umbral v=0.9)\n",
    "history_alt_2 = model_alt.fit(x_train_unlabeled[selected_v], pseudo_labels_cat[selected_v],\n",
    "                              epochs=10, batch_size=64,\n",
    "                              validation_data=(x_test, y_test_cat),\n",
    "                              verbose=2)\n",
    "\n",
    "train_loss_alt, train_acc_alt = model_alt.evaluate(x_train_labeled, y_train_labeled, verbose=0)\n",
    "test_loss_alt, test_acc_alt   = model_alt.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"Alternative Model: Train Accuracy: {:.4f} - Test Accuracy: {:.4f}\".format(train_acc_alt, test_acc_alt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Comentarios del Ejercicio 6\n",
    "\n",
    "- **Arquitectura:** Se utiliza la parte encoder del autoencoder para extraer caracter√≠sticas, a las que se a√±ade una cabeza de clasificaci√≥n.\n",
    "- **Hiperpar√°metros:** Se emplean las mismas configuraciones de optimizaci√≥n y p√©rdida que en los ejercicios anteriores.\n",
    "- **Resultados y comparaci√≥n:** Se eval√∫a el rendimiento y se compara con el resto de m√©todos implementados.\n",
    "- **Conclusiones:** Se discuten las ventajas y desventajas de la t√©cnica alternativa respecto al uso del autoencoder tradicional.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusi√≥n General\n",
    "\n",
    "En este Notebook se ha demostrado c√≥mo:\n",
    "- El entrenamiento supervisado con pocos datos puede ser limitado.\n",
    "- El auto-aprendizaje permite incorporar datos sin etiquetar y mejorar potencialmente el rendimiento.\n",
    "- Los modelos basados en autoencoder (tanto en dos pasos como en un paso) y t√©cnicas alternativas pueden extraer representaciones robustas para mejorar la clasificaci√≥n.\n",
    "- El filtrado de ejemplos at√≠picos (seg√∫n un umbral de confianza) puede ayudar a evitar el ‚Äúruido‚Äù de pseudo-etiquetas poco confiables.\n",
    "\n",
    "Cada uno de estos enfoques debe evaluarse en funci√≥n de las necesidades y limitaciones del problema concreto.\n",
    "\n",
    "> Nota: Los hiperpar√°metros, n√∫mero de √©pocas y umbrales utilizados son ejemplos. En un entorno real, se recomienda realizar un tuning exhaustivo.\n",
    "\n",
    "Este Notebook ofrece un esqueleto que puede adaptarse y ampliarse seg√∫n las necesidades de la pr√°ctica.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
